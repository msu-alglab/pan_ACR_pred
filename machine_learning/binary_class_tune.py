# Partially generated by Chat-GPT
import pandas as pd
import csv
import json

from sklearn.ensemble import (
    GradientBoostingClassifier,
    RandomForestClassifier,
    HistGradientBoostingClassifier,
    ExtraTreesClassifier
)

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import GridSearchCV

#--------------------------------------------------
#Inputs

in_dir = "/home/mwarr/Data/arabidopsis_one_genome/all_ACRs/no_cluster_random_v2_50-50"
out_file = "/home/mwarr/Data/arabidopsis_one_genome/all_ACRs/no_cluster_random_v2_50-50/tune_results.csv"

#--------------------------------------------------

# Read in data
train_df = pd.read_csv(f"{in_dir}/train.csv")
val_df = pd.read_csv(f"{in_dir}/validate.csv")

# Preprocessing: Remove ID and separate Label
# We drop the first column (index 0) because IDs are not features.
def preprocess_data(df):
    X = df.iloc[:, 1:-1]  # Take everything AFTER the first column and BEFORE the last
    y = df.iloc[:, -1]    # Take only the last column as the target
    return X, y

X_train, y_train = preprocess_data(train_df)
X_val, y_val = preprocess_data(val_df)

print(f"Feature count: {X_train.shape[1]}")

# Define models
models = {
    "HistGBC": HistGradientBoostingClassifier(max_iter=1000, early_stopping=True),
    "KNeighbors": KNeighborsClassifier(),
    "RandomForest": RandomForestClassifier(),
    "ExtraTrees": ExtraTreesClassifier(),
    "GBC": GradientBoostingClassifier()
}

# Define parameter search space
params = {
    "HistGBC": {"max_depth": [3, 5, 8, None],  "learning_rate": [.01, .1, 1]},
    "KNeighbors": {"n_neighbors": list(range(2, 40, 2)), "weights": ["distance"]},
    "RandomForest": {"n_estimators": [100, 200, 300], "max_depth": [3, 5, 8, None]},
    "ExtraTrees": {"n_estimators": [100, 200, 300], "max_depth": [3, 5, 8, None]},
    "GBC": {"max_depth": [3, 5, 8],  "learning_rate": [.1, 1, .01]}
}

# Tune each model
grid_lst = []
for model_name, model in models.items():
    print(f"Starting model {model_name}", flush=True)

    #vertically stack dataframes
    train_val_X = pd.concat([X_train, X_val]) 
    train_val_y = pd.concat([y_train, y_val])

    #specify indices for the split
    train_indices = list(range(X_train.shape[0]))
    val_indices = list(range(X_train.shape[0], train_val_X.shape[0]))
    split = [(train_indices, val_indices)] 

    gs = GridSearchCV(model, params[model_name], n_jobs=5, refit=False, cv=split, verbose=3)
    gs.fit(train_val_X, train_val_y)
    grid_lst.append((model, gs))

#Output
with open(out_file, "w", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(["model", "parameters", "test_score", "rank_test_score"])

        for model, grid in grid_lst:
            for params, mean_score, rank in zip(
                grid.cv_results_["params"],
                grid.cv_results_["mean_test_score"],
                grid.cv_results_["rank_test_score"]
            ):
                writer.writerow([
                    model,
                    json.dumps(params, sort_keys=True),  # store params as a JSON string
                    mean_score,
                    rank
                ])


